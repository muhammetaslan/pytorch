{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"  \n",
    "The torch.nn import gives us access to some helpful neural network things, such as various\n",
    "neural network layer types \n",
    "(things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc).\n",
    "\"\"\"\n",
    "import torch.nn as nn \n",
    "\"\"\"\n",
    "The torch.nn.functional area specifically gives us access to some handy functions that we might not \n",
    "want to write ourselves. We will be using the relu or \"rectified linear\" activation \n",
    "function for our neurons. Instead of writing all of the code for these things, we can just import them, \n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "All we're doing is just defining values for some layers, \n",
    "we're calling them fc1, fc2...etc, but you could call them whatever you wanted.\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  The fc just stands for fully connected\n",
    "        self.fc1 = nn.Linear(28*28,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "    \n",
    "    \"\"\"The simplest neural network is fully connected, and feed-forward, meaning we go from input to output.\n",
    "    In one side and out the other in a \"forward\" manner.  \"\"\"\n",
    "    ## forward propagation\n",
    "    def forward(self, x):\n",
    "        # her katman aktivasyon fonksiyonu ile çalışır\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # son layer da relu girmez\n",
    "        x = self.fc4(x)\n",
    "        # multi class output layer için softmax daha iyidir.\n",
    "        # binary classification için ise sigmoid()\n",
    "        # dim = 1 demek softmax daki oluşan 10 neuron için oluşan olasıkların toplamı 1 olmalı demek\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Şimdi random bir veri oluşturup kendi ağımıza input olarak vereceğiz\"\"\"\n",
    "\n",
    "X = torch.rand((28,28))\n",
    "\n",
    "# Our neural network wants this to be flattened,\n",
    "\n",
    "X = X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4470, -2.3749, -2.1682, -2.3683, -2.3114, -2.3196, -2.2070, -2.3556,\n",
       "         -2.2311, -2.2761]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
